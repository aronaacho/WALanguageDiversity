{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Impact and Limitations**\n",
    "***\n",
    "##### &emsp; Our greatest informant and limitation was our reliance on the US Census Bureau for reliable data. As 4 out of 5 of the datasets we chose to analyze were obtained from this singular, albeit grand, source, we believe that it is natural that our analysis would extend the same biases and limitations. As mentioned above in the data setting section, generally, census data is prone to response bias, with social desirability bias arguably the most susceptible to the responders as it deals with demographic data. An implication of this can be that responders of certain cultures and demographics that are less recognized in the United States may not have felt the same inclination for declaring their ancestry and language compared to those who are in communities that are actively celebrated. Especially as we look at ancestry in Washington state, we have to recognize that there are communities that had their lineage systemically erased from archives and history, such as the African and Indigenous Americans, and therefore the responders may not be able to report their full ancestral background. We also noticed the lack of AAPI ancestry in our data. With the dataset’s notes stating that “this table lists only the largest ancestry groups…” and that it only recorded the first and second ancestry, we questioned how the US Census defined “largest” and what groups made that “largest” cutoff. While they also redirected to “more detailed tables” for more ancestral groups, we agreed that this was an example of erasure, as it initially conveyed that there were no people of AAPI ancestry in Washington state. \n",
    "##### &emsp; As for our graphs, we recognize that there is skewing of the axes for a closer look at the visualizations, which can lead to unintentional misinformed opinions and understandings. While we did not see a correlation between the per pupil expenditures and the languages spoken in each county, we can see that it is apparent that there is a high proportion of Spanish speakers in comparison to the other non-English languages. This is true for the visualizations showing educational attainment by language and, although it is a loose trend, the counties with the most amount of Spanish speaker seemed to have the least amount of educational attainment. We found trends interesting, as we first thought that language spoken did not have any influence in the ways counties are supported and uplifted, however we urge viewers to think critically about other socioeconomic factors that we failed to highlight in our project that are popular indicators of wellbeing (eg. race, income). We also acknowledge that we were only able to analyze one year’s worth of data, which the reliability of our project. If we were able to invest more time into this project, we envision that we would have been able to produce much more reliable and valid results.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Challenge Goals**\n",
    "***\n",
    "\n",
    "##### &emsp; As mentioned in the data setting section, we used multiple datasets from the US Census in order to compare trends across Washington counties by connecting location based data, the Washington county, as our primary key. For example, to answer our third research question, we have taken data about the spoken language diversity of Washington counties and also data about county-specific ancestral backgrounds and have merged them together with the county being the main denominator. By using multiple datasets, we were able to achieve higher relationality and diversity in the kinds of data that were analyzed.\n",
    "### **New Libraries:** \n",
    "\n",
    "##### &emsp; We have mainly used Plotly Express and Plotly Graph Objects from the Plotly library to create interactive visualizations to support the communication of our analyses. We believe that including visualizations that have some form of interactivity with a viewer encourages more direct attention and interpretation. We especially chose to lean into the hover label feature of Plotly, which allows for the viewer to hover over a data point on a visualization and indicate its x and y values without having to physically reference the axis. We found that because we wanted to communicate relationships between features, stacking the plot on top of one another created a sense of relativity, with the hover labels helping to highlight the more granular information, allowing for data to be conveyed more transparently.\n",
    "\n",
    "### **Messy Data**:\n",
    "##### &emsp; We initially did not believe that we would have to deal with messy data, as our first glance at our datasets seemed to have been stored neatly. We quickly realized that the data from the robust US Census’s interactive interface did not translate as well when reading them as CSVs for analysis. Multi-indexes and column sections and headings were particularly messy, as we had to manually separate them with regex patterns, pivot the tables, and rename the columns to remove non-breaking and white spaces. As we were also using multiple datasets that were from collectors outside of the US Census, we also had to normalize the county and district names to make sure joins and merges of the separate tables were performed correctly without data loss.\n",
    " \n",
    "### **Result Validity**:\n",
    "##### &emsp; After looking at the results of our first visualization from our third research question, we deemed that performing a linear regression on the two variables, ancestral diversity and the number of households that spoke english exclusively, could produce a meaningful output. Although not as major as the other challenge goals, we felt that the inclusion of statistical analysis to answer this question helped strengthen our overall understanding of the relationships about language diversity within Washington state. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "***\n",
    "## **Plan Evaluation** (not done)\n",
    "***\n",
    "###### * Evaluate your proposed work plan. How accurate were your proposed work plan estimates? Why were your estimates close to reality or far from reality?\n",
    "##### &emsp; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Impact and Limitations**\n",
    "***\n",
    "##### &emsp; Our greatest informant and limitation was our reliance on the US Census Bureau for reliable data. As 4 out of 5 of the datasets we chose to analyze were obtained from this singular, albeit grand, source, we believe that it is natural that our analysis would extend the same biases and limitations. As mentioned above in the data setting section, generally, census data is prone to response bias, with social desirability bias arguably the most susceptible to the responders as it deals with demographic data. An implication of this can be that responders of certain cultures and demographics that are less recognized in the United States may not have felt the same inclination for declaring their ancestry and language compared to those who are in communities that are actively celebrated. Especially as we look at ancestry in Washington state, we have to recognize that there are communities that had their lineage systemically erased from archives and history, such as the African and Indigenous Americans, and therefore the responders may not be able to report their full ancestral background. We also noticed the lack of AAPI ancestry in our data. With the dataset’s notes stating that “this table lists only the largest ancestry groups…” and that it only recorded the first and second ancestry, we questioned how the US Census defined “largest” and what groups made that “largest” cutoff. While they also redirected to “more detailed tables” for more ancestral groups, we agreed that this was an example of erasure, as it initially conveyed that there were no people of AAPI ancestry in Washington state. \n",
    "##### &emsp; As for our graphs, we recognize that there is skewing of the axes for a closer look at the visualizations, which can lead to unintentional misinformed opinions and understandings. While we did not see a correlation between the per pupil expenditures and the languages spoken in each county, we can see that it is apparent that there is a high proportion of Spanish speakers in comparison to the other non-English languages. This is true for the visualizations showing educational attainment by language and, although it is a loose trend, the counties with the most amount of Spanish speaker seemed to have the least amount of educational attainment. We found trends interesting, as we first thought that language spoken did not have any influence in the ways counties are supported and uplifted, however we urge viewers to think critically about other socioeconomic factors that we failed to highlight in our project that are popular indicators of wellbeing (eg. race, income). We also acknowledge that we were only able to analyze one year’s worth of data, which the reliability of our project. If we were able to invest more time into this project, we envision that we would have been able to produce much more reliable and valid results.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Testing** (chec by arona)\n",
    "***\n",
    "\n",
    "##### &emsp; We tested our code by using a variety of assertations, doctests, and statistical regression evaluations. The majority of testing was done using assert statements. We compared the data values contained in each created graph with the values held in each dataframe corresponding to its respective graph. This helped ensure that no values were lost or added during the graph creation. For the cleaning functions, we used doctests and a smaller testing data file. The doctests can be found within the docstring inside each cleaning function located in the Results and Code section of this report, and the smaller data file can be found on our repository as \"small_census_testing.csv\". We know that our code computes the expected result because our assertation tests include statements to check sorted groupings of values. This not only ensures that all values we want to include in the graph are included, but also guaruntees that the values are presented in the correct order or manner. By comparing sorted values, we can make sure that the highest and lowest values are in the right place in relativity to each other. This is increasingly important as many of our visualizations rely on sorted values in order to make our conclusions and takeways more clear and concise. Additionally, we _______. The code for all of our testing (excluding the doctests found in each cleaning function) can be found below.\n",
    "\n",
    "## arona please add a sentence about the statistical analysis testing in the last cell after the \"Additionally, we ______\" part thank you!!!\n",
    "\n",
    "##### Assert statements for RQ1 code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Collaboration**\n",
    "*** \n",
    "##### &emsp; In the process of this project, we consulted the [Plotly documentation](https://plotly.com/python/) for guidance on using the Plotly library, University of Washington's Professor Ott Toomet's textbooks ([1](https://faculty.washington.edu/otoomet/machinelearning-py/linear-regression.html), [2](https://faculty.washington.edu/otoomet/machineLearning.pdf)) on Python for Machine Learning for linear regression and general statistical analysis and interpretation, and [Stack Overflow](https://stackoverflow.com/) for advice on debugging code. We have not used generative AI in any way for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Testing** (chec by arona)\n",
    "***\n",
    "\n",
    "##### &emsp; We tested our code by using a variety of assertations, doctests, and statistical regression evaluations. The majority of testing was done using assert statements. We compared the data values contained in each created graph with the values held in each dataframe corresponding to its respective graph. This helped ensure that no values were lost or added during the graph creation. For the cleaning functions, we used doctests and a smaller testing data file. The doctests can be found within the docstring inside each cleaning function located in the Results and Code section of this report, and the smaller data file can be found on our repository as \"small_census_testing.csv\". We know that our code computes the expected result because our assertation tests include statements to check sorted groupings of values. This not only ensures that all values we want to include in the graph are included, but also guaruntees that the values are presented in the correct order or manner. By comparing sorted values, we can make sure that the highest and lowest values are in the right place in relativity to each other. This is increasingly important as many of our visualizations rely on sorted values in order to make our conclusions and takeways more clear and concise. Additionally, we _______. The code for all of our testing (excluding the doctests found in each cleaning function) can be found below.\n",
    "\n",
    "## arona please add a sentence about the statistical analysis testing in the last cell after the \"Additionally, we ______\" part thank you!!!\n",
    "\n",
    "##### Assert statements for RQ1 code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppe_languages = ppe_languages.sort_values(by='ppe', ascending=False)\n",
    "all_lang_colors = ['#636EFA', '#EF553B', \"#00CC96\", \"#AB63FA\", \"#FFA15A\"]\n",
    "all_counties = []\n",
    "\n",
    "for i in range(len(ppe_languages.index)):\n",
    "    all_counties.append(str(i + 1) + \". \" + ppe_languages.index[i])\n",
    "all_lang_types = ppe_languages.iloc[:, 2:].columns\n",
    "all_lang_labels = ['Spanish', 'Other Indo-European Languages', 'Asian and Pacific Island Languages', 'Other Languages']\n",
    "fig = make_subplots(2, 1, subplot_titles=('Non-English Languages Spoken in WA Counties Ranked by School District Per Pupil Expenditure (PPE)',  \n",
    "                                          ' WA Counties Ranked by School District Per Pupil Expenditure (PPE)'))\n",
    "\n",
    "for i in range(len(all_lang_labels)):\n",
    "    fig.add_trace(go.Bar(x=all_counties,\n",
    "                    y=ppe_languages[all_lang_types[i]],\n",
    "                    name=all_lang_labels[i],\n",
    "                    marker_color=all_lang_colors[i]\n",
    "                    ), row = 1, col = 1)\n",
    "    \n",
    "fig.add_trace(go.Bar(x=all_counties,\n",
    "                y=ppe_languages['ppe'],\n",
    "                name='PPE',\n",
    "                marker_color=all_lang_colors[4]\n",
    "                ), row = 2, col = 1)\n",
    "\n",
    "fig.update_layout(autosize=False, width=1300, height=1400)\n",
    "fig['layout']['xaxis']['title']='WA Counties'\n",
    "fig['layout']['xaxis2']['title']='WA Counties'\n",
    "fig['layout']['yaxis']['title']='Percentage of Language Type Spoken'\n",
    "fig['layout']['yaxis2']['title']='Per Pupil Expenditure'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### &emsp; From this code above, we were able to generate this visualization below. To restate our motive for choosing to use two subplots was to search for any correlations between the amount of PPEs and the language spoken in a county. The PPE for each county was determined by taking the mean PPE of all K-12 school districts in the county. The subplots share the same x-axis of a PPE-ranked list of Washington state counties. To express the various language categories spoken in each county, we separated them by the non-English categories that were reported from our initial dataset: Spanish, Other Indo-European, Asian and Pacific Island, and Other languages. All of this resulted in the first visualization. When we order the counties by PPE, no clear trendline is shown from the tops of the bars. We chose to include the second subplot of the general average PPE by county for transparency and to also examine the slope in the directionality of the PPE averages.\n",
    "##### &emsp; We can see that from the highest PPE in Cowlitz County with around $12,000 to the lower PPE in Ferry County with around $6,500, there is a gradual linear decrease in PPE. Comparatively, the plot above with the languages includes various spikes of Spanish and Other Indo-European languages. Overall there is basic no correlation between these two variables. This poses the question: Should language diversity be a factor in PPE levels? From our\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pngs/ppe_by_district.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## RQ3: To what extent do Washington residents continue to practice their ancestral culture through language?\n",
    "##### [Short summary of what the code does/what we did for this question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_counties = []\n",
    "lang_counts = []\n",
    "lang_kind = []\n",
    "\n",
    "for idx, row in diverse_lang.iterrows():\n",
    "    lang_counties.append(idx)\n",
    "    lang_counts.append(row['english_only'])\n",
    "    lang_kind.append('English Only')\n",
    "    lang_counties.append(idx)\n",
    "    lang_counts.append(row['language_other_than_english'])\n",
    "    lang_kind.append('Language other than English')\n",
    "\n",
    "lang_data = {}\n",
    "lang_data['County'] = lang_counties\n",
    "lang_data['Percent'] = lang_counts\n",
    "lang_data['Language'] = lang_kind\n",
    "lang_data = pd.DataFrame(lang_data).sort_values(by='Percent')\n",
    "fig = px.bar(lang_data, x=\"Percent\", y=\"County\", color=\"Language\", title=\"Diversity in Language in WA Counties\", height=900)\n",
    "\n",
    "# let's test the strength of the relationships of the two variables: \n",
    "# diversity in each county and english only being spoken in households\n",
    "fig = px.scatter(diverse_lang, x='diversity_ratio', y='english_only', trendline='ols', \n",
    "                    title='English only being spoken in Various Ancestrially Diverse WA County',\n",
    "                    labels=dict(diversity_ratio=\"Ancestrial Diversity\", english_only=\"Households that Speak Only English\"))\n",
    "\n",
    "X = diverse_lang['english_only']\n",
    "Y = diverse_lang['diversity_ratio']\n",
    "X = sm.add_constant(X)\n",
    "m = sm.OLS(Y.astype(float), X.astype(float))\n",
    "r = m.fit()\n",
    "r.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pngs/household_diversity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Impact and Limitations**\n",
    "***\n",
    "##### &emsp; Our greatest informant and limitation was our reliance on the US Census Bureau for reliable data. As 4 out of 5 of the datasets we chose to analyze were obtained from this singular, albeit grand, source, we believe that it is natural that our analysis would extend the same biases and limitations. As mentioned above in the data setting section, generally, census data is prone to response bias, with social desirability bias arguably the most susceptible to the responders as it deals with demographic data. An implication of this can be that responders of certain cultures and demographics that are less recognized in the United States may not have felt the same inclination for declaring their ancestry and language compared to those who are in communities that are actively celebrated. Especially as we look at ancestry in Washington state, we have to recognize that there are communities that had their lineage systemically erased from archives and history, such as the African and Indigenous Americans, and therefore the responders may not be able to report their full ancestral background. We also noticed the lack of AAPI ancestry in our data. With the dataset’s notes stating that “this table lists only the largest ancestry groups…” and that it only recorded the first and second ancestry, we questioned how the US Census defined “largest” and what groups made that “largest” cutoff. While they also redirected to “more detailed tables” for more ancestral groups, we agreed that this was an example of erasure, as it initially conveyed that there were no people of AAPI ancestry in Washington state. \n",
    "##### &emsp; As for our graphs, we recognize that there is skewing of the axes for a closer look at the visualizations, which can lead to unintentional misinformed opinions and understandings. While we did not see a correlation between the per pupil expenditures and the languages spoken in each county, we can see that it is apparent that there is a high proportion of Spanish speakers in comparison to the other non-English languages. This is true for the visualizations showing educational attainment by language and, although it is a loose trend, the counties with the most amount of Spanish speaker seemed to have the least amount of educational attainment. We found trends interesting, as we first thought that language spoken did not have any influence in the ways counties are supported and uplifted, however we urge viewers to think critically about other socioeconomic factors that we failed to highlight in our project that are popular indicators of wellbeing (eg. race, income). We also acknowledge that we were only able to analyze one year’s worth of data, which the reliability of our project. If we were able to invest more time into this project, we envision that we would have been able to produce much more reliable and valid results.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Plan Evaluation** (not done)\n",
    "***\n",
    "###### * Evaluate your proposed work plan. How accurate were your proposed work plan estimates? Why were your estimates close to reality or far from reality?\n",
    "##### &emsp; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undergraduate data\n",
    "assert sorted(fig.to_dict()['data'][0]['y']) == sorted(sub_enrollment['college,_undergraduate'].tolist()), \"Undergrad data does not match expected\"\n",
    "# Postgrad data\n",
    "assert sorted(fig.to_dict()['data'][1]['y']) == sorted(sub_enrollment['graduate,_professional_school'].tolist()), \"Postgrad data does not match expected\"\n",
    "# Spanish data\n",
    "assert sorted(fig.to_dict()['data'][2]['y']) == sorted(sub_language['spanish'].tolist()), \"Spanish data does not match expected\"\n",
    "# Indo-Euro data\n",
    "assert sorted(fig.to_dict()['data'][5]['y']) == sorted(sub_language['other_indo-european_languages'].tolist()), \"Indo-European data does not match expected\"\n",
    "# Asian data\n",
    "assert sorted(fig.to_dict()['data'][8]['y']) == sorted(sub_language['asian_and_pacific_island_languages'].tolist()), \"Asian & Pacific Islander data does not match expected\"\n",
    "# Other data\n",
    "assert sorted(fig.to_dict()['data'][11]['y']) == sorted(sub_language['other_languages'].tolist()), \"Other language data does not match expected\"\n",
    "# English data\n",
    "assert sorted(fig.to_dict()['data'][14]['y']) == sorted(sub_language['speak_only_english'].tolist()), \"English data does not match expected\"\n",
    "# Other than English data\n",
    "assert sorted(fig.to_dict()['data'][17]['y']) == sorted(sub_language['speak_a_language_other_than_english'].tolist()), \"Other than English data does not match expected\"\n",
    "# Counties\n",
    "assert sorted(fig.to_dict()['data'][0]['x']) == sorted(enrollment.index.get_level_values('County')), \"Counties not accounted for\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for all counties included and are sorted by the PPE\n",
    "ppe_languages_sorted = ppe_languages.sort_values(by='ppe', ascending=False).index\n",
    "for i in range(len(fig.to_dict()['data'])):\n",
    "    all_labels = fig.to_dict()['data'][i]['x']\n",
    "    for label, idx in zip(all_labels, ppe_languages_sorted):\n",
    "        assert(label.split(' ', 1)[1] == idx), \"Data does not match expected\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Collaboration**\n",
    "*** \n",
    "##### &emsp; In the process of this project, we consulted the [Plotly documentation](https://plotly.com/python/) for guidance on using the Plotly library, University of Washington's Professor Ott Toomet's textbooks ([1](https://faculty.washington.edu/otoomet/machinelearning-py/linear-regression.html), [2](https://faculty.washington.edu/otoomet/machineLearning.pdf)) on Python for Machine Learning for linear regression and general statistical analysis and interpretation, and [Stack Overflow](https://stackoverflow.com/) for advice on debugging code. We have not used generative AI in any way for this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
